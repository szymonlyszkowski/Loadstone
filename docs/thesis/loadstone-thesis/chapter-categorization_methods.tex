\chapter{Categorization Methods}
\section{Overview}
Plain text is the most friendly and convenient in use for human beings as a data medium. Unfortunately amount of text which can be effectively analyzed by human is quite limited. On the other hand, text as source of information for computer machines does not possess any heuristics. That is the reason to develop methods which can be applied to for particular data sources selected by human. As text content is usually complex methods are designed in such way that cover narrow categories of data sources. In general methods taken under consideration in this thesis were covering following approaches:
\begin{itemize}
	\item Preprocessing - text analysis paying special attention to reject unnecessary phrases which would slow down categorizarion process.
	\item Feature Extraction - text analysis which can consider wide range of text. Useful especially to analyze human text input (internet forum comment and entries). Uses TF-IDF\footnote{Term Frequency-Inverse Document Frequency} method. The result is heuristic for further text analysis.
	\item Naive Bayesian Classifier - method enabling direct text classification to appropriate category/phrase. It requires initial knowledge (heuristics) about given input.
	\item SVM\footnote{Support Vector Machine} - a form of binary classification. It divides input to two different separable domains. In order to classify for particular domain one-to-one comparison is applied.
	\item VSM \footnote{Vector Space Model} - method of text indexing and weight measuring. Already weighted indices are easier to give a hint for human which document to query. In the last stage measures text content simillarity according to given query.
	       
\end{itemize}

\section{Preprocessing}
The most important function of preprocessing is obviously downsizing of input data. Redunant or non-heuristic data can influence factors like:
\begin{itemize}
	\item Performance - the less text to analyze the result can be obtained faster
	\item Complexity - unnecessary phrases can influence in futher steps on classification correctness (dim heuristics)
	zawartość...
\end{itemize}

The most often rejected phrases are those which appear the most frequently in particular language. Concering english language phrases like: \textit{the, a, e.g, i.e} does not introduce any hint for further classification. This is the reason why processing of such statments can be abandoned in further steps. Each document should be human analyzed properly before applying patterns to reject. Patterns will differ significantly if given input will be database entry, HTML page or plain forum comment.

\section{Feature Extraction}

Feature Extraction method as its name states allow user to extract particular parts to avoid further repetitions and introduce more uniform steps enabling human easier interpretation. Features are basically phrases which can pretend later to corresponding classes (POI categories). Phrases are dividing to two categories:
\begin{itemize}
	\item single word
	\item multi-word
\end{itemize}
To compute frequency of single words TF-IDF\cite{2}\cite{3} can be used. This method will be described later in this thesis. In case of \textbf{single word} task is not complicated because of its "non-complex nature". It is easy to detach words from each other for computer machine.
\\ \newline\textbf{Multi-word} is much more complicated construction which can consists of two or more consecutive words. Although it is difficult to clearly define how multi-world looks like, it can change meaning when atomic parts (words) are changed in order or can be translated directly. What is more human avoid repetitions of the same word when create text but when using multi-words the latter are not introduced by their counterparts to provide disambiguity. Such approach allows for better context clarity. This is the reason to deeply analyze multi-words. Multi-words can be retrieved using many different methods like: \textit{semi-supervised document classification, inspection, frequency approach, sentence comparison, mutual information}\cite{1}.

\subsection{TF-IDF}
\textbf{Term Frequency - Inverse Document Frequency} is basically about computing frequency of word occurences. When considering input data (document) it is presented as a vector of weights. Vector consists of phrases weights occuring in document. Result of TF-IDF is an information about phrase frequency occurance but in balanced way - it includes word frequency in whole collection of documents. Such approach disables local increases of importance in total (if given phrase doesn't occur relatively uniformly its weight does not increase).\\ \textbf{Term Frequency} is denoted by following formula:
\begin{equation} 
TF(term_i,doc_j)=\frac{term_{i,j}}{\sum_{0}^{k}term_{k,j}}
\end{equation}
where:\\
$term_{i,j}$ is amount of phrase (term) occurrences in given document $doc_j$\\ 
$denominator$ is sum of all phrases occurrences in document $doc_j$
\\\newline\textbf{Inverse Document Frequency} is denoted by following formula:
\begin{equation}
IDF(term_i, D) =\log{\frac{D}{1+{|d\in D : term_i \in D|}}}
\end{equation}
where:\\
$D$ is amount of documents in collection\\ 
$denominator$ is sum of all documents where $term_i$ occured
\newline\textbf{TF-IDF} is a product of both factors:
\begin{equation}
TFIDF(term_i, doc_j, D) = TF(term_i,doc_j)\times IDF(term_i,D)
\end{equation}
where:\\
$term_{i,j}$ is amount of phrase (term) occurrences in given document $doc_j$\\
$D$ is collection of documents

\subsection{Semi-supervised multi-word extraction}
Depending on the topic of input data we can try to extract multi-words using human help. As the main topic are POIs we can apply \textbf{External Glossary of Terms} or \textbf{bag of words}(BOW)\cite{1} which can consist phrases such as: "Tourist Information", "Old Town". Output of multi-word extraction can be simply compared if it is included in defined BOW. If comparison is positive multi-word is taken for further processing. To extract multi-word from given input (document) $doc_j$ we can apply rule:
\newline\textbf{Compare two sentences:}
\begin{enumerate}	
	\item Analyze each word $word_i$ in sentence $sentence_j$ of document $doc_k$.
	\item For every word $word_i$ check if is equal to word $word_l$ in sentence $sentence_m$ of document $doc_k$
	\item While $word_i$ == $word_l$ increase counter $counter_c$ by one. 
	\item If $counter_c$ is greater then one we append $word_l$ to $word_i + counter_c$ 
\end{enumerate}
Such steps will output multi-word for sentences: $sentence_j$ and $sentence_m$. To extract multi-words for document we have to for every sentence $sentence_j$ compare with $sentence_{j+1}$ while $j<k$ where $k$ is a number of sentences in document $doc_k$.

\section{Naïve Bayesian Classifier}
This method is widely used in text classification. What is important when applying this method is size of input data. Significant factor is balance between represented classes (categories) and input data (each class need to have sufficient training data). What is more, method considers complete independence between features which is not always true - \textit{'naive' word genesis e.g (analyzing data describing relation between age and income it is quite common that the older person is the income is higher).} \cite{4}
\newline When applying Naive Bayesian Classifier we have to define following factors:
\begin{itemize}
	\item Collection of documents $D = {d_0,...,d_i}$
	\item Set of words (features) $W = {w_0,...,w_j}$ - extracted previously in Feature Extraction phase
	\item Set of classes (categories) $C = {c_0,...,c_w}$
\end{itemize}  
 
First step to obtain a class to which training data are belonging is posterior probability, $P(c_j |d_i )$




